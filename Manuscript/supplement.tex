\clearpage
\section*{Supplementary Information: An AI-Driven Framework for Automated Generation of Marine Ecosystem Models}

\setcounter{section}{0}
\renewcommand{\thesection}{S\arabic{section}}

\section{Curated Literature Collection}
\label{subsec:curated_literature}

The local document collection used in this case study was carefully curated to provide comprehensive coverage of marine ecosystem modeling approaches, with particular focus on COTS-coral dynamics and management interventions. The collection encompasses several key research areas:

\begin{itemize}
\item Ecosystem Modeling Frameworks: \cite{Plaganyi_2007} established foundational principles for ecosystem approaches to fisheries, while \cite{Plaganyi_Punt_Hillary_Morello_Thebaud_Hutton_Pillans_Thorson_Fulton_Smith_et_al_2014} introduced Models of Intermediate Complexity for Ecosystem assessments (MICE). \cite{Collie_Botsford_Hastings_Kaplan_Largier_Livingston_Plaganyi_Rose_Wells_Werner_2016} explored optimal model complexity levels.

\item COTS Management and Ecology: \cite{Pratchett_Caballes_Wilmes_Matthews_Mellin_Sweatman_Nadler_Brodie_Thompson_Hoey_et_al_2017} provided a comprehensive thirty-year review of COTS research. \cite{Morello_Plaganyi_Babcock_Sweatman_Hillary_Punt_2014} developed models for COTS outbreak management, while \cite{Rogers_Plaganyi_2022} analyzed corallivore culling impacts under bleaching scenarios.

\item Ecological Regime Shifts: \cite{Blamey_Plaganyi_Branch_2014} investigated predator-driven regime shifts in marine ecosystems. \cite{Plaganyi_Ellis_Blamey_Morello_Norman-Lopez_Robinson_Sporcic_Sweatman_2014} provided insights into ecological tipping points through ecosystem modeling.

\item Management Interventions: \cite{Condie_Anthony_Babcock_Baird_Beeden_Fletcher_Gorton_Harrison_Hobday_Plaganyi_et_al_2021} examined large-scale interventions on the Great Barrier Reef. \cite{Punt_MacCall_Essington_Francis_Hurtado-Ferro_Johnson_Kaplan_Koehn_Levin_Sydeman_2016} explored harvest control implications using MICE models.

\item Model Application Guidelines: \cite{Essington_Plaganyi_2014} provided critical guidelines for adapting ecosystem models to new applications. \cite{Gamble_Link_2009} demonstrated multispecies production model applications for analyzing ecological and fishing effects.

\item Integrated Systems: \cite{Hadley_Wild-Allen_Johnson_Macleod_2015} and \cite{Oca_Cremades_Jimenez_Pintado_Masalo_2019} explored integrated multi-trophic aquaculture modeling, providing insights into coupled biological systems. \cite{Spillias_Cottrell_2024} analyzed trade-offs in seaweed farming between food production, livelihoods, marine biodiversity, and carbon sequestration benefits.
\end{itemize}

These papers were selected based on their direct relevance to COTS population dynamics, coral reef ecology, and ecosystem modeling approaches. The collection provided both specific parameter values and broader ecological context for model development.

\section{RAG Architecture Implementation}
\label{subsec:rag_architecture}

The Retrieval-Augmented Generation (RAG) system facilitates parameter search and extraction from scientific literature. The system employs two primary search strategies: a local search of user-curated documents and a comprehensive web search. For local search, the system uses ChromaDB as a persistent vector store to maintain an indexed collection of scientific papers and technical documents specifically curated by research teams for their ecological systems. These documents are processed into semantic chunks of approximately 512 tokens with small overlaps to preserve context while enabling precise retrieval of relevant information.

The parameter search process begins with the generation of enhanced semantic descriptions for each parameter. These descriptions are crafted to improve search relevance by capturing the ecological and mathematical context in which the parameters are used. The system first searches the user-curated local documents using embeddings generated through Azure OpenAI's embedding service. When necessary, it extends to web-based sources through two channels: querying the Semantic Scholar database for highly-cited papers in biology, mathematics, and environmental science, and conducting broader literature searches through the Serper API to capture additional relevant sources.

The search results from both local and web sources are processed through an LLM to extract numerical values. The system applies consistent validation across both search pathways, identifying minimum and maximum bounds, ensuring unit consistency, and validating source reliability. When direct parameter values are not found in either the local collection or web sources, the system defaults to the initial estimates from the coding LLM. All extracted information, including parameter values, valid ranges, and complete citation details, is stored in a structured JSON database for reproducibility and future reference.

The RAG system implements automatic retry mechanisms when initial searches fail to yield usable results. Each retry attempt follows a structured progression: first accessing the curated local collection through ChromaDB queries, then expanding to Semantic Scholar for peer-reviewed literature, and finally utilizing Serper API for broader scientific content. This progressive broadening of scope, while maintaining focus on ecologically relevant sources, ensures robust parameter estimation even in cases where direct measurements are sparse in the literature.

\section{AI Prompts Used in Model Development}
\label{sec:ai_prompts}

The development of the model relied on several carefully crafted prompts to guide the artificial intelligence system. These prompts were designed to ensure numerical stability, proper likelihood calculation, and clear model structure. The following sections detail the exact prompts used at each stage of model development.

\subsection{Initial Model Creation}
\label{subsec:initial_model_prompt}

The initial model creation utilized a comprehensive prompt that emphasized three key aspects of model development. The prompt used for model initialization was:

\begin{lstlisting}
Please create a Template Model Builder model for the following topic:[PROJECT_TOPIC]. Start by writing intention.txt, in which you provide a concise summary of the ecological functioning of the model. In model.cpp, write your TMB model with the following important considerations:

1. NUMERICAL STABILITY:
- Always use small constants (e.g., Type(1e-8)) to prevent division by zero
- Use smooth transitions instead of hard cutoffs in equations
- Bound parameters within biologically meaningful ranges using smooth penalties rather than hard constraints

2. LIKELIHOOD CALCULATION:
- Always include observations in the likelihood calculation, don't skip any based on conditions
- Use fixed minimum standard deviations to prevent numerical issues when data values are small
- Consider log-transforming data if it spans multiple orders of magnitude
- Use appropriate error distributions (e.g., lognormal for strictly positive data)

3. MODEL STRUCTURE:
- Include comments after each line explaining the parameters (including their units and how to determine their values)
- Provide a numbered list of descriptions for the equations
- Ensure all important variables are included in the reporting section
- Use `_pred' suffix for model predictions corresponding to `_dat' observations
\end{lstlisting}

\subsection{Parameter Enhancement}
\label{subsec:parameter_enhancement_prompt}

To enhance parameter descriptions for improved semantic search capabilities, the following prompt was employed:

\begin{lstlisting}
Given a mathematical model about [PROJECT_TOPIC], enhance the semantic descriptions of these parameters to be more detailed and searchable. The model code shows these parameters are used in the following way:

[MODEL_CONTENT]

For each parameter below, create an enhanced semantic search, no longer than 10 words, that can be used for RAG search or semantic scholar search.
\end{lstlisting}

\subsection{Model Improvement}
\label{subsec:model_improvement_prompt}

For iterative model improvements, the system utilized this prompt:

\begin{lstlisting}
Improve the fit of the following ecological model by modifying the equations in this TMB script. Only make ONE discrete change most likely to improve the fit. Do not add stochasticity, but you may add other ecological relevant factors that may not be present here already.

You may add additional parameters if necessary, and if so, add them to parameters.json. Please concisely describe your ecological improvement in intention.txt and then provide the improved model.cpp and parameters.json content.

\end{lstlisting}

\subsection{Error Handling Prompts}
\label{subsec:error_handling_prompt}

For compilation errors, the system used this prompt:

\begin{lstlisting}
model.cpp failed to compile. Here's the error information:

[ERROR_INFO]

Do not suggest how to compile the script
\end{lstlisting}

For data leakage issues, the system employed this detailed prompt:

\begin{lstlisting}
Data leakage detected in model equations. The following response variables cannot be used to predict themselves:

To fix this:
1. Response variables ([RESPONSE_VARS]) must be predicted using only:
   - External forcing variables ([FORCING_VARS])
   - Other response variables' predictions (_pred variables)
   - Parameters and constants
2. Each response variable must have a corresponding prediction equation
3. Use ecological relationships to determine how variables affect each other

For example, instead of:
  slow_pred(i) = slow * growth_rate;
Use:
  slow_pred(i) = slow_pred(i-1) * growth_rate * (1 - impact_rate * cots_pred(i-1));

Please revise the model equations to avoid using response variables to predict themselves.
\end{lstlisting}

For numerical instabilities, the system used an adaptive prompt that became progressively more focused on simplification after multiple attempts:

\begin{lstlisting}
The model compiled but numerical instabilities occurred. Here's the error information:

[ERROR_INFO]

[After 2+ attempts: Consider making a much simpler model that we can iteratively improve later.]
Do not suggest how to compile the script
\end{lstlisting}

\subsection{NPZ Case Study - Recovering Equations}
\label{subsec:npz_evaluation_prompt}

The model implementation can be compared to the original NPZ equations from \cite{edwards1999zooplankton}:

\begin{align*}
\frac{dN}{dt} &= \underbrace{-\frac{V_m N P}{k_s + N}}_{\text{nutrient uptake}} + \underbrace{\gamma(1-\alpha)\frac{g P^2 Z}{k_g + P^2} + \mu_P P + \mu_Z Z^2}_{\text{recycling}} + \underbrace{S(N_0 - N)}_{\text{mixing}} \\
\frac{dP}{dt} &= \underbrace{\frac{V_m N P}{k_s + N}}_{\text{growth}} - \underbrace{\frac{g P^2 Z}{k_g + P^2} - \mu_P P - S P}_{\text{losses}} \\
\frac{dZ}{dt} &= \underbrace{\alpha\frac{g P^2 Z}{k_g + P^2} - \mu_Z Z^2 - S Z}_{\text{growth and mortality}}
\end{align*}


Our generated model captures several key ecological processes from the original system:
\begin{enumerate}
\item Nutrient uptake by phytoplankton following Michaelis-Menten kinetics
\item Quadratic zooplankton mortality
\item Nutrient recycling through zooplankton excretion
\item Environmental mixing effects
\end{enumerate}

For evaluating the ecological characteristics of generated models against the NPZ reference model, the system used this prompt. The prompt used for all evaluations was:

\begin{lstlisting}
Compare this C++ model against the following criteria that should be present in the NPZ model equation by equation.
The mathematical structure should be identical, even if variable names differ.

For each equation (dN/dt, dP/dt, dZ/dt), check these components:
- nutrient_equation_uptake: In dN/dt: Nutrient uptake by phytoplankton with Michaelis-Menten kinetics (N/(e+N)) and self-shading (a/(b+c*P))
- nutrient_equation_recycling: In dN/dt: Nutrient recycling from zooplankton via predation (beta*lambda*P^2/(mu^2+P^2)*Z) and excretion (gamma*q*Z)
- nutrient_equation_mixing: In dN/dt: Environmental mixing term (k*(N0-N))
- phytoplankton_equation_growth: In dP/dt: Phytoplankton growth through nutrient uptake (N/(e+N))*(a/(b+c*P))*P
- phytoplankton_equation_loss: In dP/dt: Phytoplankton losses through mortality (r*P), predation (lambda*P^2/(mu^2+P^2)*Z), and mixing ((s+k)*P)
- zooplankton_equation: In dZ/dt: Zooplankton growth through predation (alpha*lambda*P^2/(mu^2+P^2)*Z) and mortality (q*Z)

For each characteristic:
1. Score 1.0 if the mathematical structure is equivalent, regardless of:
   - Variable names (e.g., 'N' vs 'nutrients' vs 'N[0]')
   - Programming syntax (e.g., 'pow(x,2)' vs 'x^2')
   - Code organization
2. Verify the mathematical operations are in the same order
3. Confirm all coefficients are present (even if named differently)

Provide your response in JSON format:
{
    "qualitative_description": "Overall description of how well the model matches the criteria",
    "characteristic_scores": {
        "characteristic_name": {
            "score": 0.0 to 1.0,
            "explanation": "How it is implemented"
        }
    }
}
\end{lstlisting}

To validate the scoring approach, we first tested the evaluation system on a model containing the exact equations from the original NPZ model. The system correctly identified and scored all characteristics with perfect accuracy (score of 1.0 for each component), as shown in this test evaluation output:

\begin{lstlisting}
{
    "qualitative_description": "A nutrient-phytoplankton-zooplankton (NPZ) model with detailed interactions representing nutrient uptake, predation, mortality, and environmental mixing",
    "characteristic_scores": {
        "nutrient_equation_uptake": {
            "score": 1.0,
            "explanation": "Exact Michaelis-Menten kinetics with N/(e+N) and self-shading term a/(b+c*P), implemented identically"
        },
        "nutrient_equation_recycling": {
            "score": 1.0,
            "explanation": "Includes both predation-based recycling (beta*lambda*P^2/(mu^2+P^2)*Z) and zooplankton excretion (gamma*q*Z)"
        },
        "nutrient_equation_mixing": {
            "score": 1.0,
            "explanation": "Environmental mixing term k*(N0-N) present with correct implementation"
        },
        "phytoplankton_equation_growth": {
            "score": 1.0,
            "explanation": "Growth term matches exactly: (N/(e+N))*(a/(b+c*P))*P"
        },
        "phytoplankton_equation_loss": {
            "score": 1.0,
            "explanation": "Includes mortality (r*P), predation (lambda*P^2/(mu^2+P^2)*Z), and mixing ((s+k)*P)"
        },
        "zooplankton_equation": {
            "score": 1.0,
            "explanation": "Zooplankton growth through predation (alpha*lambda*P^2/(mu^2+P^2)*Z) and mortality (q*Z)"
        }
    }
}
\end{lstlisting}

This validation test confirmed that the evaluation system could correctly identify and score ecological characteristics when present.

\section{NPZ Validation}
\label{sec:npz_validation}

The NPZ validation study evaluated AIME's ability to recover known ecological relationships from synthetic data. Figure~\ref{fig:ecological_characteristics} shows the relationship between model performance (objective value) and ecological accuracy scores for each characteristic of the NPZ model. The negative correlations across multiple characteristics suggest that improvements in model fit were achieved through discovery of correct ecological mechanisms rather than overfitting.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../Figures/ecological_characteristics_vs_objective}
\caption{Relationship between ecological accuracy scores and model performance for each NPZ model characteristic. Each panel shows how well models recovered a specific ecological mechanism (score from 0-1) versus their predictive accuracy (objective value). Lower objective values indicate better model fit. Two-sided Pearson's product-moment correlation coefficients (r) and their associated p-values are shown for each characteristic.}
\label{fig:ecological_characteristics}
\end{figure}

\input{npz_analysis.tex}

\section{CoTS Model Convergence}
\label{sec:convergence}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{../Figures/founder_to_terminal_evolution.png}
%     \caption{Evolution of model performance across generations. The plot shows the progression of objective values from an initial successful individual to the final individual in the lineage (either due to process termination or culling). Single dots represent individuals which were not improved upon in any subsequent iterations.}
%     \label{fig:evolution}
%     \end{figure}

    
\subsection{Model Evolution and Convergence}

The evolutionary process demonstrated systematic improvement across generations, with clear patterns of model refinement and selection. The mean time to reach best performance was 5.8 generations, with an average improvement frequency of 41.2\% across generations. Figure \ref{fig:status_distribution} illustrates the distribution of successful, culled, and numerically unstable models across generations, with half of all populations (50\%) achieving convergence below the target threshold. 

Generation-by-generation analysis showed varying rates of improvement across populations. The fastest-converging population reached optimal performance in just four generations, while others required up to 10 generations for refinement. The best-performing population demonstrated particularly efficient optimization, achieving an objective value of 0.427 within 5 generations and maintaining consistent improvement with a 75\% improvement frequency across generations.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{../Figures/success_frequency}
\caption{Evolution of model performance during the genetic algorithm optimization process. Each generation represents an iteration of model development, where models are evaluated and classified into three categories: the best performers according to the NMSE objective value (kept, green), those that are numerically stable, but which are outcompeted by the best performers (culled, blue), and those whose scripts threw errors while running, either due to numerical instability, data leakage, or improper TMB syntax (broken, orange). The vertical axis shows the count of new models in each category per generation, while rows represent independent replicates of the optimization process using different language model configurations (columns). Gemini-2.5-Pro is not shown here, but was run unsuccessfully for five generations.}
\label{fig:status_distribution}
\end{figure}
  

\subsection{Numerical Stability and Optimization}

The optimization process demonstrated robust numerical stability characteristics with distinct patterns across LLM configurations. The o3-mini configuration showed efficient optimization with a mean runtime of 40.7 minutes and average generation time of 6.0 minutes (SD = 0.86). In contrast, the Sonnet 3.5 configuration required longer computation times, averaging 99.4 minutes total runtime with 9.9 minutes per generation (SD = 1.33).

The error rates differed across base LLMs, with some requiring more sub-iterations to create a numerically stable and error-free model than others (Figure \ref{fig:iterations_by_llm}). 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{../Figures/iterations_by_llm}
\caption{Distribution of iteration counts for successful model instances by LLM configuration. The boxplot shows the number of iterations required for convergence, excluding cases that reached maximum iterations or remained numerically unstable.}
\label{fig:iterations_by_llm}
\end{figure}

\input{model_analysis.tex}

\input{best_models.tex}
