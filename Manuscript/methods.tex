At its core, AIME integrates Large Language Models (LLMs) for generating and modifying model structures, Template Model Builder (TMB) for statistical parameter estimation, and evolutionary algorithms for systematic model improvement. All of the code and data underpinning this study are available at the Github repository: \url{https://github.com/s-spillias/EMs-with-LLMs}.

\subsection{AIME Framework}

\subsubsection{Model Generation and Improvement}
AIME uses LLMs to write and modify computer code through Aider \citep{gauthier2024aider}, which is a coding assistant that can create, modify, and interpret local files. Aider can be used in the command-line or called within python scripts, as we have done here, and can receive text and/or images as input, depending on whether the underlying LLM is 'multi-modal' (i.e., can interpret text and images). Each model instance, referred to as an individual in our evolutionary framework, consists of three components: (1) a TMB-compatible dynamic model written in C++ that implements a system of equations, (2) a parameters file containing initial values and bounds, and (3) a documentation file explaining the ecological meaning of the model equations (see Section \ref{subsec:initial_model_prompt} for the complete prompt).

The LLM generates initial parameter estimates for pre-testing model structure before optimization begins. For each parameter, it assigns a priority number that determines optimization order, following established practices in ecosystem modelling \citep{Plaganyi_Punt_Hillary_Morello_Thebaud_Hutton_Pillans_Thorson_Fulton_Smith_et_al_2014}.

During non-initial steps, if multi-modal (i.e. can receive images as input) the LLM analyzes performance plots comparing predictions to historical data, otherwise the LLM receives a structured file showing the model fit residuals. After interpreting the model fit, AIME makes targeted, ecologically meaningful changes to model equations, implementing one modification at a time to maintain transparency and traceability of successful modelling strategies (see Section~\ref{subsec:model_improvement_prompt}).

\begin{landscape}
    \begin{figure}[p]
    \centering
    \includegraphics[width=0.85\linewidth]{../Figures/conceptual_diagram}
    \begingroup
    \small
    \caption{Conceptual diagram of the automated ecological modelling framework, AIME. The workflow consists of four main components: (1) User Inputs, where research questions and ecological time-series data are provided; (2) Parameterisation, utilizing RAG-enhanced literature search to estimate parameter values; (3) Model Generation/Improvement, where the Coding LLM creates new individuals with model scripts and parameters; (4) Model Execution, where the LLM's model code is implemented and TMB is used to optimise paramter values; and (5) Evolution, which evaluates model performance through individual assessment, error handling, and ranking-based selection.}
    \label{fig:conceptual}
    \endgroup
    \end{figure}
    \end{landscape}

\subsubsection{Parameterisation}
Upon initialization, the LLM estimates parameter values for each parameter supplied in the model. This initial estimation allows for the subsequent execution of the model, and the discovery of structural or syntactical errors in the LLM-generated code. If a model is successful in compiling and running, AIME goes on to find evidence to support better values for parameters. Building on the success of LLM-based extraction from ecological literature \citep{keck2025extracting,spillias2024evaluating}, the system implements a Retrieval-Augmented Generation (RAG) architecture to search scientific literature (see Section~\ref{subsec:rag_architecture} for detailed RAG implementation). Without this initial estimation step, the system risks wasting time and computational resources searching for parameter values to populate equations that may be structurally or syntactically flawed. By first validating the integrity of the model through execution, the system ensures that subsequent efforts to refine parameter values are meaningful and efficient.

The RAG process works as follows: First, the system prompts an LLM to create detailed semantic descriptions of each parameter, expanding beyond the basic descriptions provided by the coding LLM. For example, if the coding LLM defines a parameter as "growth rate of phytoplankton," the RAG system might expand this to "maximum specific growth rate of marine phytoplankton in nutrient-rich conditions, measured per day." These enhanced descriptions aim to improve the relevance of search results when querying literature databases. 

To find appropriate parameter values, the RAG system employs a structured, multi-source search strategy. First, it searches a local collection of scientific papers (see Section~\ref{subsec:curated_literature} for the complete collection used for the COTS case study) using ChromaDB \citep{Chroma2024} as a persistent vector store, with documents processed into semantic chunks to enable precise retrieval. Second, it queries the Semantic Scholar database \citep{semantic_scholar_api}. Third, it performs general web searches through the Serper API \citep{serper_api} to capture additional relevant sources. The system combines results from all three sources to build a comprehensive understanding of each parameter's possible values and ecological meaning.

The RAG system then uses LLMs to extract numerical values from the search results, determining not only parameter values but also their valid ranges. The prompt instructs the LLM to identify minimum, maximum, and typical values for each parameter, along with their units and citation information (see Section~\ref{subsec:parameter_enhancement_prompt}). All parameter information is stored in a structured JSON database that includes minimum and maximum bounds, units, and citations to source literature. For this proof of concept effort, all parameters, including those with values found from literature, are treated as estimable parameters in the optimization process. Values derived from the literature were used to bound the feasible parameter space and inform initial parameter estimates. This approach allows the optimization process to refine parameter values while still benefiting from literature-informed starting points and biologically plausible ranges.

\subsubsection{Model Execution and Error Handling}
Because LLMs often make trivial mistakes in their outputs, we developed an error handling system to address common issues. On occasion, the LLM coder will attempt to create a system of equations with circular logic (which we refer to as 'data leakage'). Data leakage occurs when the model directly uses observed values from the current time step to predict those same values, instead of properly predicting values using only information from previous time steps. To prevent this, we implement a set of code validation checks to ensure that the submitted model is properly formatted and free from logical inconsistencies.

Models are executed through Template Model Builder, TMB \citep{kristensen2014tmb}, an approach which underpins several marine ecosystem modelling frameworks \citep{chasco2021differential,albertsen2015fast,auger2017spatiotemporal}. TMB uses automatic differentiation techniques to efficiently estimate parameters and is capable of handling complex and non-linear optimisation problems. Optimisation priorities for AIME using TMB are specified in accordance with established practices \citep{Plaganyi_Punt_Hillary_Morello_Thebaud_Hutton_Pillans_Thorson_Fulton_Smith_et_al_2014,Rogers_Plaganyi_2022}. TMB's prioritization system operates through recording which parameters and intermediate calculations are actually needed for the objective function and its derivatives. This computational efficiency is particularly valuable for complex ecological models, as TMB automatically identifies which parameters influence specific likelihood components and focuses derivative calculations only on relevant parameters, significantly accelerating the optimization process.

For models that pass initial validation, AIME addresses compilation errors through automated analysis of error messages and implementation of appropriate fixes. For numerical instabilities, the system employs progressive simplification of model structure while maintaining ecological relevance. Each model variant receives up to five iterations of fixes, with later iterations favoring simpler model structures that can be iteratively improved. The specific prompts used for error handling are provided in Section~\ref{subsec:error_handling_prompt}.

\subsubsection{Model Evaluation}

For each response variable $j$, we calculate a normalized mean squared error:

\begin{equation}
    \text{NMSE}_j = \begin{cases}
        \frac{1}{n} \sum_{i=1}^{n} \left(\frac{y_{ij} - \hat{y}_{ij}}{\sigma_j}\right)^2 & \text{if } \sigma_j \neq 0 \\
        \frac{1}{n} \sum_{i=1}^{n} (y_{ij} - \hat{y}_{ij})^2 & \text{if } \sigma_j = 0
    \end{cases}
\end{equation}

where $y_{ij}$ represents observed values for variable $j$ at time $i$, $\hat{y}_{ij}$ represents corresponding model predictions, $\sigma_j$ is the unbiased standard deviation of the observed values for variable $j$ (calculated with $n-1$ denominator), and $n$ is the number of observations. The final objective function value is the mean across all response variables:

\begin{equation}
    \text{Objective} = \frac{1}{m} \sum_{j=1}^{m} \text{NMSE}_j
\end{equation}

where $m$ is the number of response variables. This approach ensures that each time series contributes equally to the objective function regardless of its scale or units. For simplicity in this proof-of-concept, we did not weight the time-series in the objective function, however this might prove useful in future work to prioritize uncovering key dynamics.

\subsubsection{Evolutionary Algorithm Implementation}

The system maintains a population of model instances, which we refer to as `individuals', where each individual represents a complete model implementation including its equations, parameters, and performance metrics. Within each generation, individuals undergo parameter optimization using Template Model Builder to find optimal parameter values for their current model structure. 

After parameter optimization, individuals are evaluated based on their prediction accuracy. Those achieving the lowest prediction errors (objective values) are selected to become parents for the next generation, while less well-performing individuals are culled and non-functioning ones (those that fail to compile or execute) are discarded. 

At the beginning of each new generation, the system creates new individuals in two ways: by making targeted structural modifications to the best-performing parent individuals from the previous generation, and by creating entirely new individuals from scratch when there are not enough functioning individuals. 

\subsection{Validation Experiments}

We conducted two complementary validation case studies of AIME. The first validation experiment aimed to see if AIME could recover known model equations from synthetic time-series data, whilst the second validation experiment examined real-world applicability through modelling a set of time-series where noise was added to the synthetic data.

\subsubsection{Retrieving Model Equations -- NPZ Case Study}

We conducted a controlled experiment using synthetic time-series data generated by a well-established nutrient-phytoplankton-zooplankton (NPZ) model from \cite{edwards1999zooplankton}, whose dynamics are well-studied \citep{boschetti2008mapping,boschetti2010detecting}. The complete system of equations is presented in Section~\ref{subsec:npz_evaluation_prompt} of the Supplementary Information. This validation tested our framework's ability to rediscover established ecological relationships from synthetic data where the underlying equations of a system are known, providing a rigorous assessment of the system's equation-learning capabilities.

In addition to monitoring the convergence of AIME's modelled time-series towards the provided time-series data, we evaluated the framework's ability to recover six key ecological characteristics from the original model, each based on a discrete term in the system of three equations: nutrient uptake by phytoplankton with Michaelis-Menten kinetics and self-shading, nutrient recycling through zooplankton predation and excretion, environmental mixing of nutrients, phytoplankton growth through nutrient uptake, phytoplankton losses through mortality and predation, and zooplankton population dynamics.

During evolution, for each `best performer' in a generation, we used Claude Sonnet-3.7 to evaluate each model and provide a score between 0 and 1 for each ecological characteristic. The scoring system was designed to be interpretable and verifiable by ecological experts. For each characteristic, the LLM was provided with:
\begin{itemize}
    \item The original equation term from the reference NPZ model
    \item The corresponding term from the generated model
    \item Specific criteria for scoring the similarity between the two terms
\end{itemize}

For example, when evaluating nutrient uptake by phytoplankton, a score of 1.0 would be assigned if the generated model correctly implemented Michaelis-Menten kinetics with self-shading (matching the form $\frac{N}{k_N + N} \cdot \frac{k_I}{k_I + P}$), while a score of 0.5 might be given if only the basic Michaelis-Menten term was present without self-shading. A score of 0 would indicate no representation of nutrient uptake. We divided the original equations into 8 terms and thus the highest possible ecological score would be 8, representing perfect agreement between the AIME-generated model and the original model equations. The complete evaluation prompt with detailed scoring criteria for each ecological characteristic is provided in Section~\ref{subsec:npz_evaluation_prompt}. This additional evaluation allowed us to better understand whether objective value improvements were indeed related to improved ecological understanding, or whether they were instead related to spurious mathematical relationships with limited ecological basis. We ran this evolutionary process using Sonnet-3.7, in four individuals for 60 generations and a convergence threshold of 0.05.

We focused on the framework's ability to recover known ecological relationships from synthetic data. We analyzed the relationship between ecological accuracy scores and objective values using two-sided Pearson's product-moment correlation tests (cor.test in R). For each ecological characteristic, we calculated the correlation coefficient (r) and tested the null hypothesis that the true correlation is 0, with the alternative hypothesis that it is not 0. The resulting p-value indicates the probability of observing such a correlation by chance if no true relationship exists, with values below 0.05 considered statistically significant. This approach allowed us to evaluate whether improvements in predictive accuracy were achieved through mathematically sound ecological mechanisms rather than through overfitting.

\subsubsection{COTS Case Study}

The Crown-of-Thorns starfish (COTS) case study examined real-world applicability through modelling populations of COTS and their prey, coral, on the Great Barrier Reef. This case study also made two external forcing time-series available to AIME, sea-surface temperature and COTS larval immigration quanitites. We tested the leading LLMs (GPT4.1, o3-mini and o4-mini from OpenAI, Claude Sonnet 3.6 and 3.7 from Anthropic, and Gemini-2.5-pro from Google) within our framework and evaluated AIME's ability to match the model created by a human expert in the same context.

The COTS model that we used as a human-derived benchmark for evaluating AIME's outputs was originally developed to specifically evaluate management interventions \citep{morello2014model}. Subsequent model versions and variations have yielded insight into management under environmental perturbations \citep{Rogers_Plaganyi_2022,Condie_Anthony_Babcock_Baird_Beeden_Fletcher_Gorton_Harrison_Hobday_Plaganyi_et_al_2021}, derivation of management thresholds \citep{plaganyi2020ecological,rogers2024validating} and their dynamic implementation \citep{rogers2023improving}. Each application differed depending on the objectives and data availability, and how the models were resolved, which required human determinations as to what was included, how it was included, and how it linked with other system aspects where necessary. Simply put, human experts were required to link management objectives and available data to resolve the necessary system aspects for informing specific management actions. 

Here we test the ability of the AI to develop a model that captures the dynamics of corals and COTS during a COTS outbreak. This not only required the AI to link the ecological dynamics but also interpret the life history characteristics of COTS to explain the observed data. Due to time and cost constraints we only performed limited tests using each LLM, where we initialized populations of four individuals for ten generations each. We calibrated these population parameters by balancing the cost of running an individual population against the rate of convergence that we found in our initial tests of the system.

We tracked several key performance metrics for each population:
\begin{itemize}
    \item Runtime performance: Total runtime and per-generation computation time
    \item Error resolution: Number of iterations required to achieve successful model implementation in each generation
    \item Model stability: Proportion of successful, culled (underperforming), and numerically unstable models per generation
\end{itemize}

We also analyzed the evolutionary trajectories of successful models by tracking their lineage from initial to final states, documenting the frequency and magnitude of improvements across generations. This included measuring the number of generations required to reach best performance and the proportion of attempts that resulted in improved models.

We also implemented a single evolution where we constructed a temporal cross-validation approach by partitioning the time series data into training (pre-2000, approximately 70\%) and testing (2000-2005, approximately 30\%) sets. This allowed us to evaluate both in-sample fit and out-of-sample prediction accuracy. For each ecosystem component (COTS abundance, fast-growing coral cover, and slow-growing coral cover), we calculated root mean square error (RMSE), mean absolute error (MAE), and R² values to quantify prediction accuracy. By comparing these metrics against those of the human-developed reference model, we could assess whether our automated approach could match expert-level performance in a real-world ecological application.


