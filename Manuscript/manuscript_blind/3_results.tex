\subsection{Retrieving Model Equations -- NPZ Case Study}


LEMMA achieved strong success in recovering the quantitative fit of the NPZ test model across all tested LLMs within ten generations. GPT-5 achieved the most accurate results, with three populations reaching objective values of 0.004, 0.007, and 0.023, indicating rapid convergence and high predictive precision (with two populations converging below 0.01 in 3 and 7 generations). Gemini-2.5-Pro showed mixed outcomes, with its best population at 0.036, but others at 0.377 and 1.881. Claude-Sonnet-4.5 delivered intermediate performance, with objective values ranging from 0.153 to 0.787. Overall, GPT-5 populations dominated in terms of convergence and error minimization, while Gemini and Claude provided reasonable approximations but with greater variability (Figure \ref{fig:npz_timeseries}).


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../Figures/NPZ_combined_with_icons.png}
\caption{Performance and ecological dynamics of NPZ model retrieval. (A) Training progress showing objective value trajectories across generations (log-scale) for GPT-5, Sonnet-4.5, and Gemini-2.5-pro. GPT-5 achieved the lowest objective value (0.0035) within three generations. (B) Time-series comparison of nutrient, phytoplankton, and zooplankton concentrations (g C m\textsuperscript{-3}) between ground-truth (black lines) and predictions from the best model by objective value (crosses). The model successfully reproduced bloom timing, nutrient drawdown, and trophic phase relationships, indicating strong alignment with ecological mechanisms.}
\label{fig:npz_timeseries}
\end{figure}

Recovery of NPZ mechanisms varied substantially across LLMs. No model individual LEMMA model perfectly reconstructed all processes, but every mechanism was recovered at least once (Figure \ref{fig:mechanism_recovery}). GPT-5 achieved the highest overall proportion of well-supported formulations (alternate from \cite{franks2002npz} or truth match), with well-supported mechanisms accounting for 80.8\% overall versus 19.2\% not supported. For the best-performing individual (GPT-5), only phytoplankton mixing was unsupported (not a truth match or alternate). With the exception of both mixing processes, all of the LLMs tended to use supported formulations. Quantitatively, the fluxes of most of the formulations used by the best-performing model were exact or very close matches to those of the truth model. However, the best-perfomer also included a detritus pathway that was not present in the ground-truth (Figure \ref{fig:npz_flux_comparison}). These patterns indicate that while overall ecological fidelity was high, certain processes, like phytoplankton and nutrient mixing were often difficult (but not impossible) for LLMs to reconstruct accurately and that quantitative matches were substantiated by ecological intention.


\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../Figures/ecology_mechanism.png}
\caption{Mechanism-level recovery of NPZ equations across LLM families. Each panel shows the distribution of raw scores for a specific ecological mechanism, where 0 = absent, 1 = present (non-standard), 2 = alternative version, and 3 = truth match. Colors represent different LLM families: Sonnet-4.5, Gemini-2.5-pro, and GPT-5. GPT-5 achieved the highest frequency of truth matches for key processes such as phytoplankton growth and zooplankton mortality, while mechanisms involving nutrient recycling and mixing were most frequently absent or represented by alternative formulations.}
\label{fig:mechanism_recovery}
\end{figure}

In addition to the presence of the ground-truth processes, we also saw that the best-performing LEMMA model successfully reproduced most ecological dynamics of the NPZ system according to the specific fluxes, although it also included additional processes that were not present in the ground truth model (Figure \ref{fig:npz_flux_comparison}). Uptake, phytoplankton growth, and zooplankton mortality exhibited near-perfect alignment with the ground-truth model, indicating that core trophic interactions were accurately reconstructed. Grazing loss and recycling processes were also well captured, though minor deviations were observed in recycling to nitrogen at higher flux values. As reported above, phytoplankton mixing was absent in the LEMMA-derived model. The best-performing model also included a detritus pathway which was not present in the original ground-truth model.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{../Figures/NPZ_form_check.png}
\caption{Comparison of process-level fluxes between the ground-truth NPZ model and best-performing LEMMA-derived model. Each panel shows the functional form of a single flux term plotted against its primary driver, with other state variables held constant at representative initial values. Optional modifiers in the compiled model (temperature, light, quadratic zooplankton mortality, detrital recycling) are disabled to isolate core equation differences. Parameter values are aligned so that curve differences reflect structural form rather than parameterization. Processes absent in one model are indicated as missing.}
\label{fig:npz_flux_comparison}
\end{figure}


\subsection{COTS Case Study}
\label{sec:cots_data}
For all tested LLMs, LEMMA was able to generate ecosystem models with prediction accuracy approaching the quantitative fit of the expert-developed model (objective value NMSE: 0.2312). After ten generations, objective values across LLMs (based on three independent runs per model) were as follows: GPT-5 achieved the best performance, with objective values ranging from 0.31-0.45 (lower is better), followed by Sonnet-4.5 (0.33-0.44) and Gemini-2.5-pro (0.47-0.69). Component-specific analysis revealed varying levels of prediction accuracy, with models showing strongest performance in predicting fast-growing coral cover and slow-growing coral cover, while maintaining reasonable accuracy for the more volatile COTS abundance patterns (Figure~\ref{fig:llm_comparison}). GPT-5 was able to correctly model the outbreaks of COTS, as requested in the initial prompt, while Gemini-2.5 Pro's best model was out of phase and Sonnet-4.5's identified the first, but not the second. 

The best-performing models generated by GPT-5, Claude Sonnet 4.5, and Gemini 2.5 Pro exhibited differences in structural design and ecological mechanisms, whilst also sharing several elements. GPT-5 aligned most closely with the demographic logic of the human reference model, while Gemini and Claude adopted more simplified, outbreak-oriented frameworks. Across all LLMs, certain core features remained consistent: density-dependent population regulation in crown-of-thorns starfish (COTS), resource limitation driven by coral availability, and differential effects on fast- versus slow-growing coral species. These commonalities occurred alongside substantial variation in life-history representation, recruitment formulation, immigration handling, mortality drivers, predation dynamics, and thermal stress responses (see Supplement%\ref{sec:model_comparison}
).

Among the LLM-generated models, GPT-5 most closely resembled the human reference model in population structure. GPT-5 modelled crown-of-thorns starfish (COTS) using two stages, juveniles and adults, while the human model applied a more detailed age-structured approach with three classes and age-specific mortality rates. In contrast, Gemini simplified the population to a single undifferentiated compartment, and Claude used a single adult compartment supplemented by episodic recruitment pulses. For recruitment, GPT-5 incorporated a Beverton-Holt-like taper on adult abundance, modified by environmental factors and lagged immigration, which paralleled the continuous Beverton-Holt density dependence used in the human model. Gemini, on the other hand, linked recruitment to coral consumption and introduced Allee effects and temperature windows without an explicit stock-recruitment curve. Claude adopted a threshold-based pulse mechanism rather than a continuous function. GPT-5 handled immigration by integrating with recruitment and applied a temporal lag, echoing the demographic embedding seen in the human model, which introduced immigration during age-0 formation with lognormal variability. Gemini instead added immigrants directly to the adult compartment, while Claude combined immigration with favorability indices and growth multipliers, creating a compound influence on population dynamics. GPT-5 and Gemini both applied baseline mortality augmented by density-dependent feedbacks, which partially reflected the human model's approach of age-specific mortality tied explicitly to prey availability. Claude introduced a starvation multiplier instead that amplified mortality when coral cover was low. For predation, GPT-5 blended Holling Type II and Type III responses, producing low-prey refuges, whereas the human model used a sigmoid saturation curve with explicit prey-switching rules. Gemini employed a standard Holling Type II functional response, and Claude incorporated prey preference weighting rather than explicit switching.
For coral growth and thermal stress, GPT-5 penalized coral growth exponentially above a thermal threshold and included a heat-loss term, which partially mirrored the human model's use of Gaussian performance curves for growth combined with logistic bleaching mortality. Gemini restricted thermal effects to logistic bleaching only, while Claude applied linear mortality beyond stress thresholds without Gaussian modulation. Finally,
temperature effects on COTS recruitment were absent from the human model but present in all LLM implementations. GPT-5 and Gemini used Gaussian temperature functions to modulate recruitment, whereas Claude embedded temperature influences within outbreak pulses and adult growth factors.

\subsection{Evolutionary Behaviour}
There were notable differences in the behaviour and speed of convergence across all LLMs tested (Supplemental Figure 1%~\ref{sec:convergence}
). The Gemini-2.5 Pro models were the fastest overall, completing ten generations in approximately 36-50 minutes, with a mean time per generation of just over four minutes. GPT-5 models showed intermediate performance, averaging about 12 minutes per generation.

Patterns of improvement also differed markedly. Sonnet-4.5 and Gemini-2.5 Pro generally produced well-functioning models early but struggled to achieve consistent gains across generations, with mean improvement rates close to zero and frequent plateaus. GPT-5 demonstrated more stable progress, including the best-performing population overall (objective value 0.0035), though most GPT-5 runs also exhibited diminishing returns after initial improvements.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{../Figures/llm_predictions_comparison.png}
    \caption{Comparison of model predictions across ecosystem components. The plots show observed versus predicted values for COTS abundance, fast-growing coral cover, and slow-growing coral cover, demonstrating the models' ability to capture key ecological patterns and relationships. Objective values (obj) shown in the legend represent the normalised mean squared error for all three variables, where lower values indicate better model performance.}
    \label{fig:llm_comparison}
\end{figure}
    

\subsubsection{Temporal Hold-Out}

Because GPT-5 demonstrated the strongest overall performance, we used it as the base LLM to test whether the LEMMA framework could create models capable of predicting out-of-sample datapoints. After ten generations, we found that the best-performing model (objective value: 0.29) had a reasonable degree of out-of-sample prediction performance on the withheld test dataset, with particularly strong predictive power for fast-growing coral cover (R² = 0.81, RMSE = 2.22, MAE = 1.92). Interestingly, this was a better objective value than the previous experiments where LEMMA had access to the entire time-series.

For slow-growing coral cover, the model achieved moderate predictive accuracy (R² = 0.17, RMSE = 1.6, MAE = 1.3), effectively capturing the general declining trend while showing some deviation in precise values. COTS population predictions demonstrated strong accuracy (R² = 0.75, RMSE = 0.07, MAE = 0.05), successfully capturing the timing and magnitude of major outbreaks.

Figure~\ref{fig:validation_combined} illustrates these prediction capabilities, showing both training period performance (pre-1997) and out-of-sample predictions (1997-2005). 
The model's ability to maintain consistent error metrics (RMSE and MAE) while capturing both rapid population dynamics and slower coral cover changes suggests it successfully identified fundamental ecological relationships governing this system.

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{../Figures/combined_validation.png}
\caption{Temporal hold-out evaluation of LEMMA driven by the best-performing LLM (GPT-5) showing predictions against observed data. LEMMA was provided with 70\% of the time-series data (pink shaded region) and developed a model that was then evaluated on the full time-series, including the remaining unseen 30\% of the time-series data (white region). Top: COTS abundance predictions showing strong capture of outbreak dynamics. Middle: Fast-growing coral cover predictions demonstrating tracking of recovery. Bottom: Slow-growing coral cover predictions illustrating general declining trends. Orange lines represent model predictions, black dots show observed data.}
\label{fig:validation_combined}
\end{figure}

\subsection{Parameter Retrieval}
\label{sec:citations_analysis}
We assessed how frequently LEMMA linked model parameters to external evidence sources, including Semantic Scholar searches and a locally curated document store. Across all generated models (21 populations, 2,619 parameters), citation integration was modest: only 3.1\% of parameters were linked to explicit citations, 1.2\% to Semantic Scholar matches, and 2.0\% to document store references. The majority of parameters lacked any external linkage.
Differences emerged between case studies. NPZ-based populations showed slightly higher citation rates (3.7\%) and semantic matches (2.7\%) than COTS-focused populations (2.8\% and 0.59\%, respectively). Document store usage was notably higher for COTS models (2.4\%) compared to NPZ (1.1\%). This disparity likely reflects the fact that the document store was curated specifically for the COTS case study, whereas no curation was performed for NPZ, limiting retrieval opportunities.
At the individual model level, citation coverage rarely exceeded 10\% of parameters, and many individuals contained no cited parameters. Where citations were present, they typically supported temperature optima, growth coefficients, and bleaching thresholds, parameters with well-documented ecological relevance. These results indicate that while LEMMA can incorporate literature-based evidence, this capability was underutilized in practice. Enhancing citation integration may require prioritizing literature-grounded parameter selection and expanding curated resources for all ecosystem contexts.
\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{../Figures/citations_boxplot.png}
\caption{Comparison of citation integration between NPZ and COTS case studies. Boxplots illustrate the proportion of model parameters with citations (Semantic Scholar or local document store). NPZ models were more reliant on Semantic Scholar because the local document store was curated specifically for the COTS case study. Asterisks represent the best performer (lowest nRMSE objective value) for each model type.}
\label{fig:citations_boxplot}
\end{figure}